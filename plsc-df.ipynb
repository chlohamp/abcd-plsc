{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main directory and target directory paths\n",
    "data_dir = \"./dset\"\n",
    "deriv_dir = \"./derivatives/none-reduced\"\n",
    "os.makedirs(deriv_dir, exist_ok=True)\n",
    "csv_sub_dir = op.join(data_dir, \"plsc-sociocult_subset_dir\")\n",
    "os.makedirs(csv_sub_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the dataframes that will be used for the main PLSC (sociocultural measures and rsFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer the Data\n",
    "#### First, we want to select the ABCD variables from the large collection of ABCD csv files.\n",
    "#### Some of the variables will be selected at specific event years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that don't need to be controlled for age\n",
    "# Define the lists of filenames and variables to extract\n",
    "abcd_var = {\n",
    "    \"abcd\": {\n",
    "        \"abcd_y_lt\": [\"interview_age\", \"rel_family_id\", \"site_id_l\"],\n",
    "        \"abcd_p_demo\": [\n",
    "            \"demo_sex_v2\",\n",
    "            \"demo_ethn_v2\",\n",
    "            #\"demo_race_a_*\",\n",
    "            \"demo_prnt_age_v2\",\n",
    "            \"demo_prnt_gender_id_v2\",\n",
    "            \"demo_prnt_ethn_v2\",\n",
    "            #\"demo_prnt_race_a_*\",\n",
    "            \"demo_prnt_ed_v2_2yr_l\",\n",
    "            \"demo_prtnr_ed_v2_2yr_l\",\n",
    "            \"demo_comb_income_v2\",\n",
    "            \"demo_prnt_income_v2_l\",\n",
    "            \"demo_origin_v2\",\n",
    "            \"demo_biomother_v2\",\n",
    "            \"demo_biofather_v2\",\n",
    "            \"demo_matgrandm_v2\",\n",
    "            \"demo_matgrandf_v2\",\n",
    "            \"demo_patgrandm_v2\",\n",
    "            \"demo_patgrandf_v2\",\n",
    "        ],\n",
    "    },\n",
    "    \"led_l\": {\n",
    "        \"led_l_coi\": [\"reshist_addr1_coi_r_coi_nat\"],\n",
    "        \"led_l_nbhsoc\": [\"reshist_addr1_nanda_disadv_fac\"],\n",
    "        \"led_l_gi\": [\"reshist_addr1_gstat_h_queen\"],\n",
    "    },\n",
    "    \"ph_y\": {\n",
    "        \"ph_y_anthro\": [\"anthroheightcalc\", \"anthroweightcalc\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# variables that need to be controlled for specific years\n",
    "# Define the lists of filenames and shared eventname\n",
    "\n",
    "abcd_var_age = {\n",
    "    \"ce_y\": {\n",
    "        \"ce_y_meim\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"meim_ss_exp\",\n",
    "                \"meim_ss_com\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_via\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"via_ss_hc\",\n",
    "                \"via_ss_amer\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_macv\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"macv_y_ss_fs\",\n",
    "                \"macv_y_ss_fo\",\n",
    "                \"macv_y_ss_fr\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_dm\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"dim_y_ss_mean\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"ce_p\": {\n",
    "        \"ce_p_meim\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"meim_p_ss_exp\",\n",
    "                \"meim_p_ss_com\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_via\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"via_p_ss_hc\",\n",
    "                \"via_p_ss_amer\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_macv\": {\n",
    "            \"eventname\": \"2_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"macv_p_ss_fs\",\n",
    "                \"macv_p_ss_fo\",\n",
    "                \"macv_p_ss_fr\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_comc\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"comc_ss_cohesion_p\",\n",
    "                \"comc_ss_control_p\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"ph_y\": {\n",
    "        \"ph_y_yrb\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"physical_activity1_y\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_resp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"resp_wheeze_yn_y\",\n",
    "                \"resp_pmcough_yn_y\",\n",
    "                \"resp_diagnosis_yn_y\",\n",
    "                \"resp_bronch_yn_y\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_mctq\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"mctq_sdweek_calc\",\n",
    "                \"mctq_msfsc_calc\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_bp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"blood_pressure_sys_mean\",\n",
    "                \"blood_pressure_dia_mean\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"mh_p\": {\n",
    "        \"mh_p_cbcl\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"cbcl_scr_syn_internal_t\",\n",
    "                \"cbcl_scr_syn_external_t\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    # here I select RS data that is available at year 4\n",
    "    \"mri_y\": {\n",
    "        \"mri_y_rsfmr_cor_gp_gp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"copy_entire_file\": True,\n",
    "        },\n",
    "        \"mri_y_adm_info\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"mri_info_manufacturer\",\n",
    "            ],\n",
    "        },\n",
    "        \"mri_y_qc_motion\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"rsfmri_meanmotion\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively find files in directories\n",
    "def find_files(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            yield os.path.join(dirpath, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through both dictionaries\n",
    "for category, files_and_vars in {**abcd_var, **abcd_var_age}.items():\n",
    "    for filename, vars_or_info in files_and_vars.items():\n",
    "        # Initialize processing parameters\n",
    "        found_file = False\n",
    "        copy_entire_file = False\n",
    "        columns_to_extract = []\n",
    "        eventname_filter = None\n",
    "\n",
    "        # Determine if it's from `abcd_var` or `abcd_var_age`\n",
    "        if isinstance(vars_or_info, dict):  # Logic for `abcd_var_age`\n",
    "            eventname_filter = vars_or_info[\"eventname\"]\n",
    "            copy_entire_file = vars_or_info.get(\"copy_entire_file\", False)\n",
    "            columns_to_extract = vars_or_info.get(\"columns_to_extract\", [])\n",
    "        else:  # Logic for `abcd_var`\n",
    "            vars_to_extract = vars_or_info\n",
    "\n",
    "        # Search for the file in all subdirectories of `data_dir`\n",
    "        for file_path in find_files(data_dir):\n",
    "            if filename + \".csv\" in file_path:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Handle processing logic based on dictionary type\n",
    "                if eventname_filter is not None:  # `abcd_var_age` logic\n",
    "                    df_filtered = df[df[\"eventname\"] == eventname_filter]\n",
    "                    if not copy_entire_file and columns_to_extract:\n",
    "                        df_filtered = df_filtered[columns_to_extract]\n",
    "                    df_subset = df_filtered\n",
    "                else:  # `abcd_var` logic\n",
    "                    # Ordered dictionary to maintain the order of columns\n",
    "                    columns_to_keep = OrderedDict()\n",
    "                    columns_to_keep[\"src_subject_id\"] = True\n",
    "                    columns_to_keep[\"eventname\"] = True\n",
    "\n",
    "                    for col in vars_to_extract:\n",
    "                        if \"*\" in col:\n",
    "                            # Use regex to match pattern\n",
    "                            regex_pattern = col.replace(\"*\", \".*\")\n",
    "                            matched_columns = list(\n",
    "                                df.filter(regex=regex_pattern).columns\n",
    "                            )\n",
    "                            for matched_col in matched_columns:\n",
    "                                columns_to_keep[matched_col] = True\n",
    "                        else:\n",
    "                            columns_to_keep[col] = True\n",
    "\n",
    "                    # Create subset dataframe with desired columns\n",
    "                    df_subset = df[list(columns_to_keep.keys())]\n",
    "\n",
    "                # Construct the destination path and filename\n",
    "                output_filename = f\"{filename}_subset.csv\"\n",
    "                output_path = os.path.join(csv_sub_dir, output_filename)\n",
    "\n",
    "                # Save the subset dataframe to the target directory\n",
    "                df_subset.to_csv(output_path, index=False)\n",
    "\n",
    "                print(f\"Saved {output_filename} to {csv_sub_dir}\")\n",
    "\n",
    "                found_file = True\n",
    "                break\n",
    "\n",
    "        if not found_file:\n",
    "            print(f\"File {filename}.csv not found in {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we want to foward fill some of the demographic data\n",
    "#### this ensures we have as much demo data as possible while making sure subject ID only appears once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the demo files\n",
    "file_names = [\n",
    "    \"abcd_y_lt_subset.csv\",\n",
    "    \"abcd_p_demo_subset.csv\",\n",
    "]\n",
    "\n",
    "# Array to store subject IDs if demo_ethn_v2 == 1 AND year 4 data available\n",
    "subject_ids = []\n",
    "\n",
    "# Loop over each file name\n",
    "for file_name in file_names:\n",
    "    # Define the path to the CSV file\n",
    "    file_path = op.join(csv_sub_dir, file_name)\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Forward fill within each 'src_subject_id' group\n",
    "    df = df.set_index(\"src_subject_id\").groupby(\"src_subject_id\").ffill().reset_index()\n",
    "\n",
    "    # Filter rows where 'eventname' is '4_year_follow_up_y_arm_1'\n",
    "    df = df[df[\"eventname\"] == \"4_year_follow_up_y_arm_1\"]\n",
    "\n",
    "    # Additional filtering for the 'abcd_p_demo_subset.csv' file\n",
    "    if file_name == \"abcd_p_demo_subset.csv\":\n",
    "        df = df[df[\"demo_ethn_v2\"] == 1]\n",
    "\n",
    "        # Save the subject IDs to the array\n",
    "        subject_ids = df[\"src_subject_id\"].tolist()\n",
    "\n",
    "    # Reset index and ensure 'src_subject_id' is preserved\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Save the processed DataFrame\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# Print the array of subject IDs\n",
    "print(\"Filtered subject IDs:\", subject_ids)\n",
    "\n",
    "# Print the count of subject IDs\n",
    "print(\n",
    "    f\"Number of subject IDs w demo_ethn_v2 == 1 AND Y4 demographic data : N = {len(subject_ids)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use subject IDS to filter CSV files\n",
    "\n",
    "rs_subject_ids = []  # Array to store unique subject IDs with RS data\n",
    "\n",
    "for file_name in os.listdir(csv_sub_dir):\n",
    "    if file_name.endswith(\".csv\"):  # Ensure we're only processing CSV files\n",
    "        file_path = os.path.join(csv_sub_dir, file_name)\n",
    "\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Keep only rows where 'src_subject_id' is in the subject_ids list\n",
    "        df_filtered = df[df[\"src_subject_id\"].isin(subject_ids)]\n",
    "\n",
    "        # Save the filtered DataFrame back to the same file\n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "\n",
    "        # print(f\"Filtered and saved {file_name}\")\n",
    "\n",
    "        if file_name == \"mri_y_rsfmr_cor_gp_gp_subset.csv\":\n",
    "            # Get the unique subject IDs\n",
    "            df_filtered = df_filtered.dropna()\n",
    "            \n",
    "            rs_subject_ids = df_filtered[\"src_subject_id\"].unique().tolist()\n",
    "\n",
    "            # Get the number of unique subject IDs\n",
    "            subject_ids_count = len(rs_subject_ids)\n",
    "\n",
    "            # Print the number of unique subject IDs\n",
    "            print(\n",
    "                f\"Number of unique subject IDs with RS data: {subject_ids_count}\"\n",
    "            )\n",
    "\n",
    "# Print the array of unique subject IDs\n",
    "print(f\"Subject IDs with RS data: {rs_subject_ids}\")\n",
    "\n",
    "for file_name in os.listdir(csv_sub_dir):\n",
    "    if file_name.endswith(\".csv\"):  # Ensure we're only processing CSV files\n",
    "        file_path = os.path.join(csv_sub_dir, file_name)\n",
    "\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Filter rows where 'src_subject_id' is in the unique RS subject IDs list\n",
    "        df_filtered = df[df[\"src_subject_id\"].isin(rs_subject_ids)]\n",
    "\n",
    "        # Save the filtered DataFrame back to the same file\n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"Filtered and saved {file_name} with RS subject IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have filtered data, we want to create measure dataframes that will be used in PLSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(csv_sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize empty DataFrames for each category\n",
    "sociocult_df_Nan = pd.DataFrame()\n",
    "covariate_df_Nan = pd.DataFrame()\n",
    "rsfc_df_Nan = pd.DataFrame()\n",
    "phyhealth_df_Nan = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function to merge CSV files on subject ID and remove 'eventname' column if it exists\n",
    "def merge_csv(file_path, df):\n",
    "    temp_df = pd.read_csv(file_path).replace([777.0, 999.0], np.nan)\n",
    "\n",
    "    # Remove 'eventname' column if it exists\n",
    "    if \"eventname\" in temp_df.columns:\n",
    "        temp_df = temp_df.drop(columns=[\"eventname\"])\n",
    "\n",
    "    # Merge on subject ID (assuming 'subject_id' is the column name for subject ID)\n",
    "    if df.empty:\n",
    "        return temp_df\n",
    "    else:\n",
    "        return pd.merge(df, temp_df, on=\"src_subject_id\", how=\"outer\")\n",
    "\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for files in os.listdir(csv_sub_dir):\n",
    "    print(files)\n",
    "    file_path = os.path.join(csv_sub_dir, files)\n",
    "\n",
    "    if files.startswith(\"ce\") or files.startswith(\"led\"):\n",
    "        sociocult_df_Nan = merge_csv(file_path, sociocult_df_Nan)\n",
    "    elif files.startswith((\"abcd\", \"mri_y_adm\", \"mri_y_qc\")):\n",
    "        covariate_df_Nan = merge_csv(file_path, covariate_df_Nan)\n",
    "    elif files.startswith(\"mri_y_rsfmr\"):\n",
    "        rsfc_df_Nan = merge_csv(file_path, rsfc_df_Nan)\n",
    "    elif files.startswith((\"mh\", \"ph\")):\n",
    "        phyhealth_df_Nan = merge_csv(file_path, phyhealth_df_Nan)\n",
    "\n",
    "# After merging, you can print or save the DataFrames as needed\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Sociocultural DataFrame:\")\n",
    "print(sociocult_df_Nan.head())\n",
    "\n",
    "print(\"\\nCovariate DataFrame:\")\n",
    "print(covariate_df_Nan.head())\n",
    "\n",
    "print(\"\\nPhysical Health DataFrame:\")\n",
    "print(phyhealth_df_Nan.head())\n",
    "\n",
    "print(\"\\nRSFC DataFrame:\")\n",
    "print(rsfc_df_Nan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving\n",
    "sociocult_save_path = os.path.join(deriv_dir, \"sociocult_Nan.csv\")\n",
    "covariate_save_path = os.path.join(deriv_dir, \"covariate_Nan.csv\")\n",
    "phyhealth_save_path = os.path.join(deriv_dir, \"phyhealth_Nan.csv\")\n",
    "rsfc_save_path = os.path.join(deriv_dir, \"rsfc_Nan.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "sociocult_df_Nan.to_csv(sociocult_save_path, index=False)\n",
    "covariate_df_Nan.to_csv(covariate_save_path, index=False)\n",
    "phyhealth_df_Nan.to_csv(phyhealth_save_path, index=False)\n",
    "rsfc_df_Nan.to_csv(rsfc_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a plot that shows percentage of missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_summary(df, df_name):\n",
    "\n",
    "    # Calculate the number of non-NaN values for each column (except the first column)\n",
    "    non_nan_counts = df.iloc[:, 1:].notna().sum()\n",
    "    # Calculate the percentage of missing data for each column (except the first column)\n",
    "    missing_percentage = df.iloc[:, 1:].isna().mean() * 100\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Measure\": df.columns[1:],  # Exclude the first column\n",
    "            \"Number of Subjects\": non_nan_counts,  # Number of non-NaN values\n",
    "            \"Percentage Missing\": missing_percentage,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Determine if an asterisk should be added\n",
    "    summary[\"Above 5%\"] = summary[\"Percentage Missing\"].apply(\n",
    "        lambda x: \"*\" if x > 5 else \"\"\n",
    "    )\n",
    "\n",
    "    # Create a formatted string for the DataFrame summary\n",
    "    summary_str = f\"\\n{df_name} Missing Data Summary:\\n\"\n",
    "    summary_str += (\n",
    "        summary.to_string(\n",
    "            index=False,\n",
    "            columns=[\"Measure\", \"Number of Subjects\", \"Percentage Missing\", \"Above 5%\"],\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    )\n",
    "\n",
    "    return summary, summary_str\n",
    "\n",
    "\n",
    "# Generate missing data summaries for each DataFrame\n",
    "sociocult_summary, sociocult_summary_str = missing_data_summary(\n",
    "    sociocult_df_Nan, \"Sociocultural DataFrame\"\n",
    ")\n",
    "covariate_summary, covariate_summary_str = missing_data_summary(\n",
    "    covariate_df_Nan, \"Covariate DataFrame\"\n",
    ")\n",
    "phyhealth_summary, phyhealth_summary_str = missing_data_summary(\n",
    "    phyhealth_df_Nan, \"Physical Health DataFrame\"\n",
    ")\n",
    "rsfc_summary, rsfc_summary_str = missing_data_summary(rsfc_df_Nan, \"RSFC DataFrame\")\n",
    "\n",
    "# Combine all summaries into one string\n",
    "all_summaries = (\n",
    "    sociocult_summary_str\n",
    "    + covariate_summary_str\n",
    "    + phyhealth_summary_str\n",
    "    + rsfc_summary_str\n",
    ")\n",
    "\n",
    "# Write the combined summaries to a text file\n",
    "with open(\"PLSC-missing_data.txt\", \"w\") as file:\n",
    "    file.write(all_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(color_codes=True)\n",
    "\n",
    "\n",
    "def plot_missing_data_summary(summary, df_name, deriv_dir):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    barplot = sns.barplot(\n",
    "        x=\"Percentage Missing\", y=\"Measure\", data=summary, palette=\"viridis\"\n",
    "    )\n",
    "    plt.title(f\"{df_name} Percentage of Missing Data\")\n",
    "    plt.xlabel(\"Percentage Missing (out of 589 subjects)\")\n",
    "    plt.ylabel(f\"ABCD {df_name} Measures\")\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    # Add vertical lines\n",
    "    # plt.axvline(x=5, color=\"green\", linestyle=\"--\", label=\"5% Missing\")\n",
    "    # plt.axvline(x=10, color=\"red\", linestyle=\"--\", label=\"10% Missing\")\n",
    "\n",
    "    # Add custom legend with total number of subjects\n",
    "    plt.legend()\n",
    "\n",
    "    for index, row in summary.iterrows():\n",
    "        percentage_missing = format(row[\"Percentage Missing\"], \".2f\")\n",
    "        text_color = \"red\" if row[\"Percentage Missing\"] > 5 else \"black\"\n",
    "        barplot.annotate(\n",
    "            f\"{percentage_missing}%\",\n",
    "            xy=(row[\"Percentage Missing\"], index),\n",
    "            xytext=(20, 0),  # Move text slightly more to the right\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=text_color,\n",
    "        )\n",
    "\n",
    "    # Save the figure\n",
    "    file_path = os.path.join(deriv_dir, f\"{df_name.replace(' ', '_')}_missing_data.png\")\n",
    "    plt.tight_layout()  # Ensure a tight layout with some padding\n",
    "    #plt.savefig(file_path)\n",
    "    #plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot and save the bar plots\n",
    "plot_missing_data_summary(sociocult_summary, \"Sociocultural\", deriv_dir)\n",
    "plot_missing_data_summary(covariate_summary, \"Covariate\", deriv_dir)\n",
    "plot_missing_data_summary(phyhealth_summary, \"Physical Health\", deriv_dir)\n",
    "#plot_missing_data_summary(rsfc_summary, \"RSFC DataFrame\", deriv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will remove all subjects with missing data to make final csvs for the PLSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify covariates we want to include\n",
    "\n",
    "cov_list = [\n",
    "    \"src_subject_id\",\n",
    "    \"interview_age\",\n",
    "    \"rel_family_id\",\n",
    "    \"site_id_l\",\n",
    "    \"demo_sex_v2\",\n",
    "    \"demo_prnt_age_v2\",\n",
    "    \"demo_prnt_gender_id_v2\",\n",
    "    \"demo_prnt_ed_v2_2yr_l\",\n",
    "    \"demo_prtnr_ed_v2_2yr_l\",\n",
    "    \"demo_comb_income_v2\",\n",
    "    \"demo_prnt_income_v2_l\",\n",
    "    \"demo_origin_v2\",\n",
    "    \"mri_info_manufacturer\",\n",
    "    \"rsfmri_meanmotion\",\n",
    "]\n",
    "covariate_df_Nan_filtered = covariate_df_Nan[cov_list]\n",
    "\n",
    "# Define the list of sites to exclude\n",
    "exclude_sites = [\"site07\", \"site14\", \"site15\"]\n",
    "\n",
    "# Filter the DataFrame to exclude rows with the specified sites\n",
    "'''covariate_df_Nan_filtered = covariate_df_Nan_filtered[\n",
    "    ~covariate_df_Nan_filtered[\"site_id_l\"].isin(exclude_sites)\n",
    "]'''\n",
    "\n",
    "\n",
    "# specify sociocult we want to include\n",
    "soc_list = [\n",
    "    \"src_subject_id\",\n",
    "    \"meim_ss_exp\",\n",
    "    \"meim_ss_com\",\n",
    "    \"via_ss_hc\",\n",
    "    \"via_ss_amer\",\n",
    "    \"macv_y_ss_fs\",\n",
    "    \"macv_y_ss_fo\",\n",
    "    \"macv_y_ss_fr\",\n",
    "    \"meim_p_ss_exp\",\n",
    "    \"meim_p_ss_com\",\n",
    "    \"via_p_ss_hc\",\n",
    "    \"via_p_ss_amer\",\n",
    "    \"macv_p_ss_fs\",\n",
    "    \"macv_p_ss_fo\",\n",
    "    \"macv_p_ss_fr\",\n",
    "    \"dim_y_ss_mean\",\n",
    "    \"comc_ss_cohesion_p\",\n",
    "    \"comc_ss_control_p\",\n",
    "]\n",
    "\n",
    "# Filter out rows where 'comc_ss_control_p' is greater than 100\n",
    "sociocult_df_Nan = sociocult_df_Nan[sociocult_df_Nan[\"comc_ss_control_p\"] <= 100]\n",
    "\n",
    "sociocult_df_Nan_filtered = sociocult_df_Nan #[soc_list]\n",
    "\n",
    "# Original list\n",
    "rsfc_list = [\n",
    "    \"src_subject_id\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_ad\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_cgc\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_ca\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_n\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_ad_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_cgc\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_ca\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_n\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_cgc_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_ca\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_n\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_ca_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_n\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_n\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_dla_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_fo\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_n\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_fo_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_n_ngd_n\",\n",
    "    \"rsfmri_c_ngd_n_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_n_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_n_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_n_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_n_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_n_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_rspltp\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_rspltp_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_sa_ngd_sa\",\n",
    "    \"rsfmri_c_ngd_sa_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_sa_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_sa_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_sa_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_smh_ngd_smh\",\n",
    "    \"rsfmri_c_ngd_smh_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_smh_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_smh_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_smm_ngd_smm\",\n",
    "    \"rsfmri_c_ngd_smm_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_smm_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_vta_ngd_vta\",\n",
    "    \"rsfmri_c_ngd_vta_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_vs_ngd_vs\",\n",
    "]\n",
    "\n",
    "# Filter the list to exclude items containing \"_ad\", \"_smh\", \"_smm\", or \"_vs\"\n",
    "rsfc_list = [\n",
    "    var\n",
    "    for var in rsfc_list\n",
    "    if all(keyword not in var for keyword in [\"_ad\", \"_smh\", \"_smm\", \"_vs\", \"d_n\"])\n",
    "]\n",
    "\n",
    "#Filter out only none network\n",
    "'''rsfc_list = [\n",
    "    var\n",
    "    for var in rsfc_list\n",
    "    if all(keyword not in var for keyword in [\"d_n\"])\n",
    "]'''\n",
    "\n",
    "print(\"Number of Filtered Variables:\", len(rsfc_list))\n",
    "\n",
    "rsfc_df_Nan_filtered = rsfc_df_Nan[rsfc_list]\n",
    "\n",
    "phyhealth_df_Nan_filtered = phyhealth_df_Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any NaN values\n",
    "sociocult_df_cleaned = sociocult_df_Nan_filtered.dropna()\n",
    "covariate_df_cleaned = covariate_df_Nan_filtered.dropna()\n",
    "rsfc_df_cleaned = rsfc_df_Nan_filtered.dropna()\n",
    "phyhealth_df_cleaned = phyhealth_df_Nan_filtered.dropna()\n",
    "\n",
    "print(sociocult_df_cleaned)\n",
    "print(covariate_df_cleaned)\n",
    "print(rsfc_df_cleaned)\n",
    "print(phyhealth_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of subject IDs in both DataFrames\n",
    "# Get the intersection of subject IDs in all three DataFrames\n",
    "common_ids = (\n",
    "    set(sociocult_df_cleaned[\"src_subject_id\"])\n",
    "    .intersection(set(covariate_df_cleaned[\"src_subject_id\"]))\n",
    "    .intersection(set(rsfc_df_cleaned[\"src_subject_id\"]))\n",
    ")\n",
    "print(common_ids)\n",
    "\n",
    "common_ids.discard(\"NDAR_INV6EPGL57G\")\n",
    "\n",
    "# Count the number of common IDs\n",
    "print(\"Number of common subject IDs:\", len(common_ids))\n",
    "\n",
    "# Filter both DataFrames to keep only the rows with common subject IDs\n",
    "sociocult_df_final = sociocult_df_cleaned[\n",
    "    sociocult_df_cleaned[\"src_subject_id\"].isin(common_ids)\n",
    "]\n",
    "sociocult_df_final = sociocult_df_final.drop(columns=[\"src_subject_id\"])\n",
    "\n",
    "covariate_df_final = covariate_df_cleaned[\n",
    "    covariate_df_cleaned[\"src_subject_id\"].isin(common_ids)\n",
    "]\n",
    "rsfc_df_final = rsfc_df_cleaned[rsfc_df_cleaned[\"src_subject_id\"].isin(common_ids)]\n",
    "# Drop the src_subject_id column from rsfc_df_final\n",
    "rsfc_df_nosub = rsfc_df_final.drop(columns=[\"src_subject_id\"])\n",
    "\n",
    "\n",
    "print(sociocult_df_final)\n",
    "print(covariate_df_final)\n",
    "print(rsfc_df_final)\n",
    "print(rsfc_df_nosub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving\n",
    "sociocult_save_path = os.path.join(deriv_dir, \"sociocult.csv\")\n",
    "covariate_save_path = os.path.join(deriv_dir, \"covariate.csv\")\n",
    "rsfc_save_path = os.path.join(deriv_dir, \"rsfc-sub.csv\")\n",
    "rsfcnosub_save_path = os.path.join(deriv_dir, \"rsfc.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "sociocult_df_final.to_csv(sociocult_save_path, index=False)\n",
    "covariate_df_final.to_csv(covariate_save_path, index=False)\n",
    "rsfc_df_final.to_csv(rsfc_save_path, index=False)\n",
    "rsfc_df_nosub.to_csv(rsfcnosub_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, I want to combine some measures for the reduced PLSC\n",
    "\n",
    "#### to get to 8 variables:\n",
    "##### meim_y = (meim_ss_exp + meim_ss_com) / 2\n",
    "##### via_y = (via_ss_hc + via_ss_amer) / 2\n",
    "##### macv_y = (macv_y_ss_fs + macv_y_ss_fo + macv_y_ss_fr) / 3\n",
    "##### meim_p = (meim_p_ss_exp + meim_p_ss_com) / 2\n",
    "##### via_p = (via_p_ss_hc + via_p_ss_amer) / 2\n",
    "##### macv_p = (macv_p_ss_fs + macv_p_ss_fo + macv_p_ss_fr) / 3\n",
    "##### dim_y_ss_mean  \n",
    "##### comc_p = (comc_ss_cohesion_p, comc_ss_control_p) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Create a new DataFrame with the calculated columns\n",
    "new_sociocult_df = pd.DataFrame()\n",
    "\n",
    "# Assign the calculated columns to the new DataFrame\n",
    "new_sociocult_df[\"meim_y\"] = (\n",
    "    sociocult_df_final[\"meim_ss_exp\"] + sociocult_df_final[\"meim_ss_com\"]\n",
    ") / 2\n",
    "new_sociocult_df[\"via_y\"] = (\n",
    "    sociocult_df_final[\"via_ss_hc\"] + sociocult_df_final[\"via_ss_amer\"]\n",
    ") / 2\n",
    "new_sociocult_df[\"macv_y\"] = (\n",
    "    sociocult_df_final[\"macv_y_ss_fs\"]\n",
    "    + sociocult_df_final[\"macv_y_ss_fo\"]\n",
    "    + sociocult_df_final[\"macv_y_ss_fr\"]\n",
    ") / 3\n",
    "new_sociocult_df[\"meim_p\"] = (\n",
    "    sociocult_df_final[\"meim_p_ss_exp\"] + sociocult_df_final[\"meim_p_ss_com\"]\n",
    ") / 2\n",
    "new_sociocult_df[\"via_p\"] = (\n",
    "    sociocult_df_final[\"via_p_ss_hc\"] + sociocult_df_final[\"via_p_ss_amer\"]\n",
    ") / 2\n",
    "new_sociocult_df[\"macv_p\"] = (\n",
    "    sociocult_df_final[\"macv_p_ss_fs\"]\n",
    "    + sociocult_df_final[\"macv_p_ss_fo\"]\n",
    "    + sociocult_df_final[\"macv_p_ss_fr\"]\n",
    ") / 3\n",
    "new_sociocult_df[\"dim_y_ss_mean\"] = sociocult_df_final[\n",
    "    \"dim_y_ss_mean\"\n",
    "]  # Assuming dim_y_ss_mean is already in your DataFrame\n",
    "new_sociocult_df[\"comc_p\"] = (\n",
    "    sociocult_df_final[\"comc_ss_cohesion_p\"] + sociocult_df_final[\"comc_ss_control_p\"]\n",
    ") / 2\n",
    "\n",
    "\n",
    "print(new_sociocult_df)\n",
    "\n",
    "sociocult_save_path = os.path.join(deriv_dir, \"sociocult_8.csv\")\n",
    "new_sociocult_df.to_csv(sociocult_save_path, index=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the dataframes for the Regression Analysis (physical health, rsFC)\n",
    "\n",
    "#### Sig Dimensions:\n",
    "##### Exluding the none network : Dim 1, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dir = op.join(deriv_dir, \"regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add back the subject id column so we remove the correct rows in the next step\n",
    "\n",
    "# x is rsfc, y is sociocult\n",
    "latent_df = pd.read_csv(op.join(deriv_dir, \"rniXrsfc_lx-base.csv\"))\n",
    "\n",
    "latent_df[\"src_subject_id\"] = rsfc_df_final[\"src_subject_id\"].values\n",
    "\n",
    "print(latent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of subject IDs in both DataFrames for the Physical Health regression analysis\n",
    "# Get the intersection of subject IDs in all three DataFrames\n",
    "phyhealth_common_ids = (\n",
    "    set(covariate_df_final[\"src_subject_id\"])\n",
    "    .intersection(set(phyhealth_df_cleaned[\"src_subject_id\"]))\n",
    ")\n",
    "\n",
    "\n",
    "# Filter both DataFrames to keep only the rows with common subject IDs\n",
    "phyhealth_df_regfinal = phyhealth_df_cleaned[\n",
    "    phyhealth_df_cleaned[\"src_subject_id\"].isin(phyhealth_common_ids)\n",
    "]\n",
    "\n",
    "rsfc_df_regfinal = rsfc_df_final[\n",
    "    rsfc_df_final[\"src_subject_id\"].isin(phyhealth_common_ids)\n",
    "]\n",
    "\n",
    "latent_df_regfinal = latent_df[latent_df[\"src_subject_id\"].isin(phyhealth_common_ids)]\n",
    "\n",
    "covariate_df_regfinal = covariate_df_final[\n",
    "    covariate_df_final[\"src_subject_id\"].isin(phyhealth_common_ids)\n",
    "]\n",
    "\n",
    "print(phyhealth_df_regfinal)\n",
    "print(rsfc_df_regfinal)\n",
    "print(latent_df_regfinal)\n",
    "print(covariate_df_regfinal)\n",
    "\n",
    "# Define file paths for saving\n",
    "phyhealth_reg_save_path = os.path.join(reg_dir, \"phyhealth-reg.csv\")\n",
    "rsfc_reg_save_path = os.path.join(reg_dir, \"rsfc-reg.csv\")\n",
    "latent_reg_save_path = os.path.join(reg_dir, \"latent-reg.csv\")\n",
    "covariate_reg_save_path = os.path.join(reg_dir, \"covariate-reg.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "phyhealth_df_regfinal.to_csv(phyhealth_reg_save_path, index=False)\n",
    "rsfc_df_regfinal.to_csv(rsfc_reg_save_path, index=False)\n",
    "latent_df_regfinal.to_csv(latent_reg_save_path, index=False)\n",
    "covariate_df_regfinal.to_csv(covariate_reg_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we have any siblings that we need to account for in the LME\n",
    "# 1) How many total entries and how many unique sites?\n",
    "total = covariate_df_regfinal.shape[0]\n",
    "unique = covariate_df_regfinal[\"rel_family_id\"].nunique()\n",
    "print(f\"Total rows: {total}, Unique sites: {unique}\")\n",
    "\n",
    "# 2) Are there any repeats?\n",
    "num_duplicates = total - unique\n",
    "print(f\"Number of repeated rel_family_id entries: {num_duplicates}\")\n",
    "\n",
    "# 3) If you want to see which site IDs repeat and how often:\n",
    "counts = covariate_df_regfinal[\"rel_family_id\"].value_counts()\n",
    "repeats = counts[counts > 1]\n",
    "print(\"Repeated site IDs and their counts:\")\n",
    "print(repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded regression dataframes and filtered anthro_df to match phyhealth_reg_df:\n",
      "phyhealth_reg_df shape: (171, 15)\n",
      "rsfc_reg_df shape: (171, 67)\n",
      "latent_reg_df shape: (171, 22)\n",
      "covariate_reg_df shape: (171, 14)\n",
      "anthro_df rows: before=40524, after=171\n",
      "anthro_df columns: ['src_subject_id', 'anthroweightcalc', 'anthroheightcalc', 'BMI']\n",
      "Merged columns into phyhealth_reg_df: ['anthroheightcalc', 'anthroweightcalc']\n",
      "Non-null counts in phyhealth_reg_df (after merge): {'BMI': 171, 'anthroweightcalc': 171, 'anthroheightcalc': 171}\n"
     ]
    }
   ],
   "source": [
    "phyhealth_reg_save_path = os.path.join(reg_dir, \"phyhealth-reg.csv\")\n",
    "rsfc_reg_save_path = os.path.join(reg_dir, \"rsfc-reg.csv\")\n",
    "latent_reg_save_path = os.path.join(reg_dir, \"latent-reg.csv\")\n",
    "covariate_reg_save_path = os.path.join(reg_dir, \"covariate-reg.csv\")\n",
    "anthro_save_path = os.path.join(reg_dir, \"ph_y_anthro.csv\")\n",
    "\n",
    "# Load the regression CSVs\n",
    "phyhealth_reg_df = pd.read_csv(phyhealth_reg_save_path)\n",
    "rsfc_reg_df = pd.read_csv(rsfc_reg_save_path)\n",
    "latent_reg_df = pd.read_csv(latent_reg_save_path)\n",
    "covariate_reg_df = pd.read_csv(covariate_reg_save_path)\n",
    "anthro_df = pd.read_csv(anthro_save_path)\n",
    "\n",
    "# Reduce anthro_df to subjects present in phyhealth_reg_df\n",
    "phy_ids = set(phyhealth_reg_df[\"src_subject_id\"].unique())\n",
    "rows_before = len(anthro_df)\n",
    "anthro_df = anthro_df[anthro_df[\"src_subject_id\"].isin(phy_ids)].copy()\n",
    "\n",
    "# Keep only eventname = 4_year_follow_up_y_arm_1\n",
    "if \"eventname\" in anthro_df.columns:\n",
    "    anthro_df = anthro_df[anthro_df[\"eventname\"] == \"4_year_follow_up_y_arm_1\"].copy()\n",
    "else:\n",
    "    print(\"Warning: anthro_df missing 'eventname' column; cannot filter by event.\")\n",
    "\n",
    "# Keep only the requested columns in anthro_df\n",
    "anthro_keep_cols = [\"src_subject_id\", \"anthroweightcalc\", \"anthroheightcalc\"]\n",
    "missing_cols = [c for c in anthro_keep_cols if c not in anthro_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: anthro_df is missing columns: {missing_cols}\")\n",
    "else:\n",
    "    anthro_df = anthro_df[anthro_keep_cols].copy()\n",
    "\n",
    "# Compute BMI using height in meters: BMI = kg / m^2\n",
    "# Convert height from centimeters to meters before squaring (ABCD anthropometrics are typically in cm)\n",
    "weight_kg = pd.to_numeric(anthro_df[\"anthroweightcalc\"], errors=\"coerce\")\n",
    "height_cm = pd.to_numeric(anthro_df[\"anthroheightcalc\"], errors=\"coerce\")\n",
    "height_m = height_cm / 100.0\n",
    "anthro_df[\"BMI\"] = weight_kg / (height_m ** 2)\n",
    "# Replace inf/-inf from divide-by-zero with NaN\n",
    "anthro_df.loc[~np.isfinite(anthro_df[\"BMI\"]), \"BMI\"] = np.nan\n",
    "\n",
    "rows_after = len(anthro_df)\n",
    "\n",
    "# Merge weight, height, and BMI into phyhealth_reg_df by src_subject_id\n",
    "merge_cols = [\"src_subject_id\", \"anthroweightcalc\", \"anthroheightcalc\", \"BMI\"]\n",
    "anthro_merge = anthro_df[merge_cols].drop_duplicates(subset=\"src_subject_id\")\n",
    "prev_cols = set(phyhealth_reg_df.columns)\n",
    "phyhealth_reg_df = phyhealth_reg_df.merge(anthro_merge, on=\"src_subject_id\", how=\"left\", suffixes=(\"\", \"_anthro\"))\n",
    "\n",
    "# Coalesce overlapping columns to avoid *_x/*_y style duplicates\n",
    "for col in [\"anthroweightcalc\", \"anthroheightcalc\", \"BMI\"]:\n",
    "    right_col = f\"{col}_anthro\"\n",
    "    if right_col in phyhealth_reg_df.columns:\n",
    "        if col in phyhealth_reg_df.columns:\n",
    "            phyhealth_reg_df[col] = phyhealth_reg_df[col].combine_first(phyhealth_reg_df[right_col])\n",
    "            phyhealth_reg_df.drop(columns=[right_col], inplace=True)\n",
    "        else:\n",
    "            phyhealth_reg_df.rename(columns={right_col: col}, inplace=True)\n",
    "\n",
    "added_cols = list(set(phyhealth_reg_df.columns) - prev_cols)\n",
    "counts = {\n",
    "    \"BMI\": int(phyhealth_reg_df.get(\"BMI\").notna().sum()) if \"BMI\" in phyhealth_reg_df.columns else 0,\n",
    "    \"anthroweightcalc\": int(phyhealth_reg_df.get(\"anthroweightcalc\").notna().sum()) if \"anthroweightcalc\" in phyhealth_reg_df.columns else 0,\n",
    "    \"anthroheightcalc\": int(phyhealth_reg_df.get(\"anthroheightcalc\").notna().sum()) if \"anthroheightcalc\" in phyhealth_reg_df.columns else 0,\n",
    "}\n",
    "\n",
    "print(\"Loaded regression dataframes and filtered anthro_df to match phyhealth_reg_df:\")\n",
    "print(f\"phyhealth_reg_df shape: {phyhealth_reg_df.shape}\")\n",
    "print(f\"rsfc_reg_df shape: {rsfc_reg_df.shape}\")\n",
    "print(f\"latent_reg_df shape: {latent_reg_df.shape}\")\n",
    "print(f\"covariate_reg_df shape: {covariate_reg_df.shape}\")\n",
    "print(f\"anthro_df rows: before={rows_before}, after={rows_after}\")\n",
    "print(f\"anthro_df columns: {list(anthro_df.columns)}\")\n",
    "print(f\"Merged columns into phyhealth_reg_df: {sorted(added_cols)}\")\n",
    "print(\"Non-null counts in phyhealth_reg_df (after merge):\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyhealth_reg_df\n",
    "\n",
    "# Define file paths for saving\n",
    "phyhealth_reg_save_path = os.path.join(reg_dir, \"phyhealth-reg.csv\")\n",
    "\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "phyhealth_reg_df.to_csv(phyhealth_reg_save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_cgc-dt_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_dt-dla_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_dt-dt_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_dt-vs_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_vs-vs_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim3/phyhealth_cgc-dt_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim3/phyhealth_dt-dla_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim3/phyhealth_dt-dt_data.csv\n"
     ]
    }
   ],
   "source": [
    "# create df for the corr coeff reg analysis\n",
    "# rsfc measures that came back as signficant\n",
    "# will make a df for each of these that has all of the covariates and phy health measures\n",
    "\n",
    "dim1_rsfc_reg_measures = [\n",
    "    \"rsfmri_c_ngd_cgc_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_vs\",\n",
    "    \"rsfmri_c_ngd_vs_ngd_vs\",\n",
    "]\n",
    "dim1_rsfc_short_labels = [\"cgc-dt\", \"dt-dla\", \"dt-dt\", \"dt-vs\", \"vs-vs\"]\n",
    "\n",
    "dim3_rsfc_reg_measures = [\n",
    "    \"rsfmri_c_ngd_cgc_ngd_dt\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dla\",\n",
    "    \"rsfmri_c_ngd_dt_ngd_dt\"\n",
    "]\n",
    "dim3_rsfc_short_labels = [\"cgc-dt\", \"dt-dla\", \"dt-dt\"]\n",
    "\n",
    "# Get the unique subject IDs from phyhealth_reg_df\n",
    "phyhealth_subjects = phyhealth_reg_df[\"src_subject_id\"].unique()\n",
    "\n",
    "# Loop through both dimensions\n",
    "for dim_name, measures, labels in [(\"dim1\", dim1_rsfc_reg_measures, dim1_rsfc_short_labels),\n",
    "                                    (\"dim3\", dim3_rsfc_reg_measures, dim3_rsfc_short_labels)]:\n",
    "    # Create dimension-specific subdirectory\n",
    "    dim_reg_dir = os.path.join(reg_dir, dim_name)\n",
    "    os.makedirs(dim_reg_dir, exist_ok=True)\n",
    "    \n",
    "    for measure, short_label in zip(measures, labels):\n",
    "        # 1) pull out subj‑ID + measure, then rename to \"rsfc\"\n",
    "        tmp = rsfc_reg_df[[\"src_subject_id\", measure]].copy()\n",
    "        tmp.rename(columns={measure: \"rsfc\"}, inplace=True)\n",
    "\n",
    "        # 2) filter to only include subjects in phyhealth_reg_df\n",
    "        tmp = tmp[tmp[\"src_subject_id\"].isin(phyhealth_subjects)]\n",
    "\n",
    "        # 3) merge in covariates + phys‑health\n",
    "        tmp = tmp.merge(covariate_reg_df, on=\"src_subject_id\", how=\"inner\")\n",
    "        tmp = tmp.merge(phyhealth_reg_df, on=\"src_subject_id\", how=\"inner\")\n",
    "\n",
    "        # 4) write out\n",
    "        out_path = os.path.join(dim_reg_dir, f\"phyhealth_{short_label}_data.csv\")\n",
    "        tmp.to_csv(out_path, index=False)\n",
    "        print(f\"Wrote {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ./derivatives/none-reduced/regression/dim1/phyhealth_dim1_data.csv\n",
      "Wrote ./derivatives/none-reduced/regression/dim3/phyhealth_dim3_data.csv\n"
     ]
    }
   ],
   "source": [
    "# create df for the latent score reg analysis\n",
    "# rsfc measures that came back as signficant\n",
    "# will make a df for each of these that has all of the covariates and phy health measures\n",
    "\n",
    "latent_reg_dimensions = [\"V1\", \"V3\"]\n",
    "dimension_labels = [\"dim1\", \"dim3\"]\n",
    "\n",
    "for dim, dimension_label in zip(latent_reg_dimensions, dimension_labels):\n",
    "\n",
    "    # 1) pull out subj‑ID + measure, then rename to \"score\"\n",
    "    tmp = latent_reg_df[[\"src_subject_id\", dim]].copy()\n",
    "    tmp.rename(columns={dim: \"score\"}, inplace=True)  # latent score\n",
    "\n",
    "    # 2) merge in covariates + phys‑health\n",
    "    tmp = tmp.merge(covariate_reg_df, on=\"src_subject_id\", how=\"inner\")\n",
    "    tmp = tmp.merge(phyhealth_reg_df, on=\"src_subject_id\", how=\"inner\")\n",
    "\n",
    "    # 3) write out to dimension-specific subfolder\n",
    "    dim_reg_dir = os.path.join(reg_dir, dimension_label)\n",
    "    os.makedirs(dim_reg_dir, exist_ok=True)\n",
    "    out_path = os.path.join(dim_reg_dir, f\"phyhealth_{dimension_label}_data.csv\")\n",
    "    tmp.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Wrote {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
